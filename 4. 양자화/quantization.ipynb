{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEd3vqVP-d3F",
        "outputId": "8fc1414b-1b8f-479a-ae58-b2ee8f7786b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "MERGED_MODEL_PATH: /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4\n",
            "GGUF_OUT_PATH    : /content/exaone35-2.4b-Q4_K_M.gguf\n",
            "GGUF_BACKUP_PATH : /content/drive/MyDrive/data/test2/exaone35-2.4b-Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# === Google Drive 마운트 ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === 경로 설정 ===\n",
        "MERGED_MODEL_PATH =   \"/content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4\"\n",
        "\n",
        "# 출력 GGUF 파일(Colab 로컬)\n",
        "GGUF_OUT_PATH = \"/content/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "\n",
        "# GGUF 백업 파일(Drive)\n",
        "GGUF_BACKUP_PATH = \"/content/drive/MyDrive/data/test2/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "GGUF_F16 = \"/content/exaone35-2.4b-f16.gguf\"\n",
        "print(\"MERGED_MODEL_PATH:\", MERGED_MODEL_PATH)\n",
        "print(\"GGUF_OUT_PATH    :\", GGUF_OUT_PATH)\n",
        "print(\"GGUF_BACKUP_PATH :\", GGUF_BACKUP_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqn7r5uH__JV",
        "outputId": "1c437a4f-0616-437b-d4e1-998e5a18d52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA: 12.6 | GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF4SQL5pAC0r",
        "outputId": "3a322731-d499-48df-e2f1-515babeacf38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4'에서 최종 병합 모델을 로드합니다...\n",
            "✅ lm_head.weight tied to output embeddings.\n",
            "오늘은 어떤 날이냐? 오늘은 2013년 10월 26일, 제20대 국회에서 '국가인권위원회법'이 제정되었던 날이었노라. 이는 우리 사회의 인권 수준을 한 단계 높이는 중요한 발걸음이었느니라. 이 법의 제정\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f\"'{MERGED_MODEL_PATH}'에서 최종 병합 모델을 로드합니다...\")\n",
        "\n",
        "final_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MERGED_MODEL_PATH,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "final_tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)\n",
        "\n",
        "# 패딩 설정(경고 방지)\n",
        "final_tokenizer.padding_side = \"left\"\n",
        "if final_tokenizer.pad_token is None:\n",
        "    final_tokenizer.pad_token = final_tokenizer.eos_token\n",
        "\n",
        "# (필요 시) lm_head tie 워크어라운드\n",
        "try:\n",
        "    if hasattr(final_model, \"get_output_embeddings\") and hasattr(final_model, \"lm_head\"):\n",
        "        final_model.lm_head.weight = final_model.get_output_embeddings().weight\n",
        "        print(\"✅ lm_head.weight tied to output embeddings.\")\n",
        "except Exception as e:\n",
        "    print(\"lm_head tie skip:\", e)\n",
        "\n",
        "# 간단 생성 테스트\n",
        "prompt = \"오늘은 어떤 날이냐?\"\n",
        "inputs = final_tokenizer(prompt, return_tensors=\"pt\").to(final_model.device)\n",
        "outputs = final_model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
        "print(final_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDWMlMVW-6lr",
        "outputId": "52fb4032-b4ef-4898-93a5-9090c8e50d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 64532, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 64532 (delta 127), reused 64 (delta 53), pack-reused 64312 (from 3)\u001b[K\n",
            "Receiving objects: 100% (64532/64532), 172.86 MiB | 40.37 MiB/s, done.\n",
            "Resolving deltas: 100% (46841/46841), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ llama.cpp 및 변환 의존성 준비 완료\n"
          ]
        }
      ],
      "source": [
        "# llama.cpp 가져오기 + 변환 스크립트 의존성\n",
        "!git clone https://github.com/ggml-org/llama.cpp\n",
        "!pip -q install -r llama.cpp/requirements.txt\n",
        "print(\"✅ llama.cpp 및 변환 의존성 준비 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tlIPG7VAGbK",
        "outputId": "b14b61eb-bb18-47c8-c98b-f201d97f199b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:hf-to-gguf:Loading model: sft_tuned_model_epoch_4\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4: The repository /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4 contains custom code which must be executed to correctly load the model. You can inspect the repository content at /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4 .\n",
            " You can inspect the repository content at https://hf.co//content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4.\n",
            "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:Model architecture: ExaoneForCausalLM\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4: The repository /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4 contains custom code which must be executed to correctly load the model. You can inspect the repository content at /content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4 .\n",
            " You can inspect the repository content at https://hf.co//content/drive/MyDrive/data/test2/experiments_lora/r16_a32_do0.05_lr0.0003/sft_tuned_model_epoch_4.\n",
            "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {40}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2560, 2560}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2560, 640}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2560, 7168}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {7168, 2560}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2560}\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2560, 102400}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 101782 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 361\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and message['role'] != 'system' %}{{ '[|system|][|endofturn|]\n",
            "' }}{% endif %}{{ '[|' + message['role'] + '|]' + message['content'] }}{% if message['role'] == 'user' %}{{ '\n",
            "' }}{% else %}{{ '[|endofturn|]\n",
            "' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '[|assistant|]' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/exaone35-2.4b-f16.gguf: n_tensors = 273, total_size = 4.8G\n",
            "Writing: 100% 4.81G/4.81G [00:09<00:00, 496Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/exaone35-2.4b-f16.gguf\n",
            "f16 GGUF 생성 여부: True -> /content/exaone35-2.4b-f16.gguf\n"
          ]
        }
      ],
      "source": [
        "!python llama.cpp/convert_hf_to_gguf.py \\\n",
        "  --outfile \"{GGUF_F16}\" \\\n",
        "  --outtype f16 \\\n",
        "  \"{MERGED_MODEL_PATH}\"\n",
        "\n",
        "import os\n",
        "print(\"f16 GGUF 생성 여부:\", os.path.exists(GGUF_F16), \"->\", GGUF_F16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbKijNGXDh9D",
        "outputId": "0d0ecf3b-b839-45d1-da0c-3d1d44c98733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llama.cpp\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "\u001b[33mCMake Warning at CMakeLists.txt:126 (message):\n",
            "  LLAMA_NATIVE is deprecated and will be removed in the future.\n",
            "\n",
            "  Use GGML_NATIVE instead\n",
            "\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:135 (llama_option_depr)\n",
            "\n",
            "\u001b[0m\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.4\n",
            "-- ggml commit:  cdb6da46\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.7s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  7%] Built target build_info\n",
            "[  7%] Built target sha1\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 10%] Built target sha256\n",
            "[ 10%] Built target llama-minicpmv-cli\n",
            "[ 10%] Built target llama-qwen2vl-cli\n",
            "[ 10%] Built target llama-llava-cli\n",
            "[ 10%] Built target llama-gemma3-cli\n",
            "[ 10%] Built target xxhash\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[ 10%] Built target ggml-base\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 16%] Built target ggml-cpu\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 16%] Built target ggml\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 28%] Built target llama-gguf\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 28%] Built target llama-gguf-hash\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 28%] Built target llama\n",
            "[ 28%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 35%] Built target test-c\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 36%] Built target llama-simple\n",
            "[ 36%] Built target llama-simple-chat\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 37%] Built target mtmd\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 73%] Built target test-mtmd-c-api\n",
            "[ 73%] Built target test-model-load-cancel\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 73%] Built target test-rope\n",
            "[ 73%] Built target test-log\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 73%] Built target test-autorelease\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 75%] Built target test-gbnf-validator\n",
            "[ 75%] Built target test-quantize-fns\n",
            "[ 75%] Built target test-barrier\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 76%] Built target llama-lookup-merge\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 76%] Built target test-tokenizer-1-bpe\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 76%] Built target test-tokenizer-1-spm\n",
            "[ 76%] Built target llama-logits\n",
            "[ 76%] Built target llama-q8dot\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 76%] Built target llama-tokenize\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 77%] Built target llama-vdot\n",
            "[ 77%] Built target llama-gguf-split\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 79%] Built target test-alloc\n",
            "[ 79%] Built target test-thread-safety\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 79%] Built target llama-lookup\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 82%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 83%] Built target test-grammar-parser\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 85%] Built target test-tokenizer-0\n",
            "[ 85%] Built target test-sampling\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 86%] Built target llama-embedding\n",
            "[ 86%] Built target llama-lookup-stats\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 86%] Built target llama-batched-bench\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 89%] Built target test-llama-grammar\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 89%] Built target llama-eval-callback\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 89%] Built target llama-lookup-create\n",
            "[ 89%] Built target llama-batched\n",
            "[ 89%] Built target llama-passkey\n",
            "[ 89%] Built target test-quantize-perf\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 90%] Built target test-opt\n",
            "[ 90%] Built target llama-gen-docs\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 91%] Built target llama-speculative-simple\n",
            "[ 91%] Built target llama-quantize\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 91%] Built target llama-save-load-state\n",
            "[ 91%] Built target llama-finetune\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 91%] Built target test-gguf\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 91%] Built target llama-parallel\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 91%] Built target test-regex-partial\n",
            "[ 91%] Built target llama-retrieval\n",
            "[ 91%] Built target llama-lookahead\n",
            "[ 91%] Built target test-arg-parser\n",
            "[ 91%] Built target llama-mtmd-cli\n",
            "[ 91%] Built target test-chat-template\n",
            "[ 91%] Built target llama-export-lora\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 92%] Built target test-json-partial\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 93%] Built target llama-cvector-generator\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 94%] Built target llama-diffusion-cli\n",
            "[ 94%] Built target llama-speculative\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 94%] Built target llama-cli\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 94%] Built target test-quantize-stats\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 95%] Built target llama-run\n",
            "[ 95%] Built target test-grammar-integration\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 96%] Built target test-chat-parser\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 97%] Built target test-json-schema-to-grammar\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 98%] Built target llama-imatrix\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 99%] Built target llama-bench\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 99%] Built target llama-tts\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[100%] Built target test-chat\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[100%] Built target test-backend-ops\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "total 87388\n",
            "drwxr-xr-x  2 root root    4096 Oct 10 12:21 .\n",
            "drwxr-xr-x 12 root root    4096 Oct 10 12:19 ..\n",
            "-rwxr-xr-x  1 root root  732600 Oct 10 12:19 libggml-base.so\n",
            "-rwxr-xr-x  1 root root 1005560 Oct 10 12:19 libggml-cpu.so\n",
            "-rwxr-xr-x  1 root root   55232 Oct 10 12:19 libggml.so\n",
            "-rwxr-xr-x  1 root root 2614368 Oct 10 12:19 libllama.so\n",
            "-rwxr-xr-x  1 root root  790872 Oct 10 12:20 libmtmd.so\n",
            "-rwxr-xr-x  1 root root 2470016 Oct 10 12:20 llama-batched\n",
            "-rwxr-xr-x  1 root root 2470112 Oct 10 12:20 llama-batched-bench\n",
            "-rwxr-xr-x  1 root root  527584 Oct 10 12:21 llama-bench\n",
            "-rwxr-xr-x  1 root root 2509880 Oct 10 12:20 llama-cli\n",
            "-rwxr-xr-x  1 root root  356952 Oct 10 12:20 llama-convert-llama2c-to-ggml\n",
            "-rwxr-xr-x  1 root root 2503312 Oct 10 12:20 llama-cvector-generator\n",
            "-rwxr-xr-x  1 root root 2483328 Oct 10 12:20 llama-diffusion-cli\n",
            "-rwxr-xr-x  1 root root 2479320 Oct 10 12:20 llama-embedding\n",
            "-rwxr-xr-x  1 root root 2470320 Oct 10 12:20 llama-eval-callback\n",
            "-rwxr-xr-x  1 root root 2503888 Oct 10 12:20 llama-export-lora\n",
            "-rwxr-xr-x  1 root root 2466168 Oct 10 12:20 llama-finetune\n",
            "-rwxr-xr-x  1 root root   16904 Oct 10 12:19 llama-gemma3-cli\n",
            "-rwxr-xr-x  1 root root 2470248 Oct 10 12:20 llama-gen-docs\n",
            "-rwxr-xr-x  1 root root   28128 Oct 10 12:19 llama-gguf\n",
            "-rwxr-xr-x  1 root root  103448 Oct 10 12:19 llama-gguf-hash\n",
            "-rwxr-xr-x  1 root root   48152 Oct 10 12:20 llama-gguf-split\n",
            "-rwxr-xr-x  1 root root 2569560 Oct 10 12:21 llama-imatrix\n",
            "-rwxr-xr-x  1 root root   16904 Oct 10 12:19 llama-llava-cli\n",
            "-rwxr-xr-x  1 root root  328336 Oct 10 12:20 llama-logits\n",
            "-rwxr-xr-x  1 root root 2474712 Oct 10 12:20 llama-lookahead\n",
            "-rwxr-xr-x  1 root root 2499760 Oct 10 12:20 llama-lookup\n",
            "-rwxr-xr-x  1 root root 2490936 Oct 10 12:20 llama-lookup-create\n",
            "-rwxr-xr-x  1 root root   74768 Oct 10 12:20 llama-lookup-merge\n",
            "-rwxr-xr-x  1 root root 2495864 Oct 10 12:20 llama-lookup-stats\n",
            "-rwxr-xr-x  1 root root   16904 Oct 10 12:19 llama-minicpmv-cli\n",
            "-rwxr-xr-x  1 root root 2493200 Oct 10 12:20 llama-mtmd-cli\n",
            "-rwxr-xr-x  1 root root 2482976 Oct 10 12:20 llama-parallel\n",
            "-rwxr-xr-x  1 root root 2470112 Oct 10 12:20 llama-passkey\n",
            "-rwxr-xr-x  1 root root 2563512 Oct 10 12:20 llama-perplexity\n",
            "-rwxr-xr-x  1 root root   21192 Oct 10 12:20 llama-q8dot\n",
            "-rwxr-xr-x  1 root root  372560 Oct 10 12:20 llama-quantize\n",
            "-rwxr-xr-x  1 root root   16904 Oct 10 12:19 llama-qwen2vl-cli\n",
            "-rwxr-xr-x  1 root root 2488056 Oct 10 12:20 llama-retrieval\n",
            "-rwxr-xr-x  1 root root 1996136 Oct 10 12:20 llama-run\n",
            "-rwxr-xr-x  1 root root 2470560 Oct 10 12:20 llama-save-load-state\n",
            "-rwxr-xr-x  1 root root 4302120 Oct 10 12:21 llama-server\n",
            "-rwxr-xr-x  1 root root   27088 Oct 10 12:19 llama-simple\n",
            "-rwxr-xr-x  1 root root   32456 Oct 10 12:19 llama-simple-chat\n",
            "-rwxr-xr-x  1 root root 2498240 Oct 10 12:20 llama-speculative\n",
            "-rwxr-xr-x  1 root root 2487656 Oct 10 12:20 llama-speculative-simple\n",
            "-rwxr-xr-x  1 root root  324080 Oct 10 12:20 llama-tokenize\n",
            "-rwxr-xr-x  1 root root 2578616 Oct 10 12:21 llama-tts\n",
            "-rwxr-xr-x  1 root root   21776 Oct 10 12:20 llama-vdot\n",
            "-rwxr-xr-x  1 root root   50328 Oct 10 12:20 test-alloc\n",
            "-rwxr-xr-x  1 root root 2479136 Oct 10 12:20 test-arg-parser\n",
            "-rwxr-xr-x  1 root root   18144 Oct 10 12:20 test-autorelease\n",
            "-rwxr-xr-x  1 root root  852168 Oct 10 12:21 test-backend-ops\n",
            "-rwxr-xr-x  1 root root   22160 Oct 10 12:20 test-barrier\n",
            "-rwxr-xr-x  1 root root   15776 Oct 10 12:19 test-c\n",
            "-rwxr-xr-x  1 root root 2110992 Oct 10 12:21 test-chat\n",
            "-rwxr-xr-x  1 root root 1880120 Oct 10 12:21 test-chat-parser\n",
            "-rwxr-xr-x  1 root root 1883768 Oct 10 12:20 test-chat-template\n",
            "-rwxr-xr-x  1 root root   26864 Oct 10 12:20 test-gbnf-validator\n",
            "-rwxr-xr-x  1 root root   77256 Oct 10 12:20 test-gguf\n",
            "-rwxr-xr-x  1 root root  739544 Oct 10 12:20 test-grammar-integration\n",
            "-rwxr-xr-x  1 root root   41072 Oct 10 12:20 test-grammar-parser\n",
            "-rwxr-xr-x  1 root root  236328 Oct 10 12:20 test-json-partial\n",
            "-rwxr-xr-x  1 root root  736848 Oct 10 12:21 test-json-schema-to-grammar\n",
            "-rwxr-xr-x  1 root root   46336 Oct 10 12:20 test-llama-grammar\n",
            "-rwxr-xr-x  1 root root   43128 Oct 10 12:20 test-log\n",
            "-rwxr-xr-x  1 root root   16512 Oct 10 12:20 test-model-load-cancel\n",
            "-rwxr-xr-x  1 root root   16944 Oct 10 12:20 test-mtmd-c-api\n",
            "-rwxr-xr-x  1 root root   68128 Oct 10 12:20 test-opt\n",
            "-rwxr-xr-x  1 root root   17568 Oct 10 12:20 test-quantize-fns\n",
            "-rwxr-xr-x  1 root root   41656 Oct 10 12:20 test-quantize-perf\n",
            "-rwxr-xr-x  1 root root  217376 Oct 10 12:20 test-quantize-stats\n",
            "-rwxr-xr-x  1 root root  396584 Oct 10 12:20 test-regex-partial\n",
            "-rwxr-xr-x  1 root root   21472 Oct 10 12:20 test-rope\n",
            "-rwxr-xr-x  1 root root   50272 Oct 10 12:20 test-sampling\n",
            "-rwxr-xr-x  1 root root 2470968 Oct 10 12:20 test-thread-safety\n",
            "-rwxr-xr-x  1 root root  339072 Oct 10 12:20 test-tokenizer-0\n",
            "-rwxr-xr-x  1 root root  320432 Oct 10 12:20 test-tokenizer-1-bpe\n",
            "-rwxr-xr-x  1 root root  320168 Oct 10 12:20 test-tokenizer-1-spm\n"
          ]
        }
      ],
      "source": [
        "%cd /content/llama.cpp\n",
        "\n",
        "!cmake -S . -B build -DLLAMA_NATIVE=ON\n",
        "!cmake --build build -j\n",
        "!ls -al build/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZuMdv_RDh6c",
        "outputId": "8ff55a55-7d99-4ede-ffda-d191e056e3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Found quantize binaries: ['/content/llama.cpp/build/bin/llama-quantize']\n",
            "✅ Using quantize: /content/llama.cpp/build/bin/llama-quantize\n",
            "\n",
            "=== 결과 확인 ===\n",
            "Q4_K_M GGUF 생성: True -> /content/exaone35-2.4b-Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "import os, glob, subprocess\n",
        "\n",
        "GGUF_F16 = \"/content/exaone35-2.4b-f16.gguf\"\n",
        "GGUF_Q4  = \"/content/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "\n",
        "# quantize 실행파일 자동 탐색\n",
        "quant_bins = glob.glob(\"/content/llama.cpp/**/llama-quantize\", recursive=True)\n",
        "print(\"🔍 Found quantize binaries:\", quant_bins)\n",
        "\n",
        "if not quant_bins:\n",
        "    raise FileNotFoundError(\"quantize 실행 파일을 찾지 못했습니다. build/bin 또는 bin/ 아래를 확인하세요.\")\n",
        "\n",
        "quant_path = quant_bins[0]\n",
        "print(f\"✅ Using quantize: {quant_path}\")\n",
        "\n",
        "# 실행 권한 부여 후 양자화 실행\n",
        "os.system(f\"chmod +x '{quant_path}'\")\n",
        "os.system(f\"'{quant_path}' '{GGUF_F16}' '{GGUF_Q4}' q4_K_M\")\n",
        "\n",
        "print(\"\\n=== 결과 확인 ===\")\n",
        "print(\"Q4_K_M GGUF 생성:\", os.path.exists(GGUF_Q4), \"->\", GGUF_Q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpwqGDvDFVUx",
        "outputId": "f68fbe0e-cda1-49d3-ba01-77da5066542c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main: build = 6727 (cdb6da46)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/exaone35-2.4b-f16.gguf' to '/content/exaone35-2.4b-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 273 tensors from /content/exaone35-2.4b-f16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = exaone\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Sft_Tuned_Model_Epoch_4\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 2.4B\n",
            "llama_model_loader: - kv   4:                    exaone.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   5:                exaone.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   6:             exaone.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   7:                      exaone.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:    exaone.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                 exaone.feed_forward_length u32              = 7168\n",
            "llama_model_loader: - kv  10:                         exaone.block_count u32              = 30\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                      exaone.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:                exaone.rope.dimension_count u32              = 80\n",
            "llama_model_loader: - kv  14:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = exaone\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,102400]  = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\", ...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,102400]  = [3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,101782]  = [\"t h\", \"Ġ a\", \"Ġ í\", \"i n\", \"Ġ t...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 361\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - type  f32:   62 tensors\n",
            "llama_model_loader: - type  f16:  211 tensors\n",
            "[   1/ 273]                   output_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[   2/ 273]                    rope_freqs.weight - [   40,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
            "[   3/ 273]                    token_embd.weight - [ 2560, 102400,     1,     1], type =    f16, converting to q6_K .. size =   500.00 MiB ->   205.08 MiB\n",
            "[   4/ 273]                  blk.0.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[   5/ 273]               blk.0.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[   6/ 273]             blk.0.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[   7/ 273]                  blk.0.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[   8/ 273]                  blk.0.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[   9/ 273]                blk.0.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[  10/ 273]                blk.0.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  11/ 273]                blk.0.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  12/ 273]                  blk.0.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  13/ 273]                  blk.1.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  14/ 273]               blk.1.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  15/ 273]             blk.1.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  16/ 273]                  blk.1.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  17/ 273]                  blk.1.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[  18/ 273]                blk.1.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[  19/ 273]                blk.1.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  20/ 273]                blk.1.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  21/ 273]                  blk.1.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  22/ 273]                  blk.2.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  23/ 273]               blk.2.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  24/ 273]             blk.2.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  25/ 273]                  blk.2.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  26/ 273]                  blk.2.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[  27/ 273]                blk.2.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[  28/ 273]                blk.2.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  29/ 273]                blk.2.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  30/ 273]                  blk.2.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  31/ 273]                  blk.3.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  32/ 273]               blk.3.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  33/ 273]             blk.3.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  34/ 273]                  blk.3.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  35/ 273]                  blk.3.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  36/ 273]                blk.3.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  37/ 273]                blk.3.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  38/ 273]                blk.3.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  39/ 273]                  blk.3.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  40/ 273]                  blk.4.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  41/ 273]               blk.4.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  42/ 273]             blk.4.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  43/ 273]                  blk.4.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  44/ 273]                  blk.4.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  45/ 273]                blk.4.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  46/ 273]                blk.4.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  47/ 273]                blk.4.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  48/ 273]                  blk.4.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  49/ 273]                  blk.5.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  50/ 273]               blk.5.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  51/ 273]             blk.5.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  52/ 273]                  blk.5.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  53/ 273]                  blk.5.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[  54/ 273]                blk.5.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[  55/ 273]                blk.5.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  56/ 273]                blk.5.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  57/ 273]                  blk.5.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  58/ 273]                  blk.6.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  59/ 273]               blk.6.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  60/ 273]             blk.6.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  61/ 273]                  blk.6.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  62/ 273]                  blk.6.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  63/ 273]                blk.6.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  64/ 273]                blk.6.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  65/ 273]                blk.6.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  66/ 273]                  blk.6.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  67/ 273]                  blk.7.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  68/ 273]               blk.7.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  69/ 273]             blk.7.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  70/ 273]                  blk.7.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  71/ 273]                  blk.7.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  72/ 273]                blk.7.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  73/ 273]                blk.7.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  74/ 273]                blk.7.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  75/ 273]                  blk.7.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  76/ 273]                  blk.8.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  77/ 273]               blk.8.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  78/ 273]             blk.8.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  79/ 273]                  blk.8.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  80/ 273]                  blk.8.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[  81/ 273]                blk.8.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[  82/ 273]                blk.8.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  83/ 273]                blk.8.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  84/ 273]                  blk.8.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  85/ 273]                  blk.9.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  86/ 273]               blk.9.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  87/ 273]             blk.9.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  88/ 273]                  blk.9.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  89/ 273]                  blk.9.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  90/ 273]                blk.9.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  91/ 273]                blk.9.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  92/ 273]                blk.9.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  93/ 273]                  blk.9.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[  94/ 273]                 blk.10.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  95/ 273]              blk.10.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[  96/ 273]            blk.10.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  97/ 273]                 blk.10.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  98/ 273]                 blk.10.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[  99/ 273]               blk.10.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 100/ 273]               blk.10.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 101/ 273]               blk.10.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 102/ 273]                 blk.10.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 103/ 273]                 blk.11.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 104/ 273]              blk.11.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 105/ 273]            blk.11.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 106/ 273]                 blk.11.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 107/ 273]                 blk.11.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 108/ 273]               blk.11.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 109/ 273]               blk.11.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 110/ 273]               blk.11.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 111/ 273]                 blk.11.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 112/ 273]                 blk.12.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 113/ 273]              blk.12.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 114/ 273]            blk.12.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 115/ 273]                 blk.12.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 116/ 273]                 blk.12.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 117/ 273]               blk.12.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 118/ 273]               blk.12.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 119/ 273]               blk.12.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 120/ 273]                 blk.12.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 121/ 273]                 blk.13.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 122/ 273]              blk.13.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 123/ 273]            blk.13.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 124/ 273]                 blk.13.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 125/ 273]                 blk.13.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 126/ 273]               blk.13.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 127/ 273]               blk.13.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 128/ 273]               blk.13.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 129/ 273]                 blk.13.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 130/ 273]                 blk.14.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 131/ 273]              blk.14.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 132/ 273]            blk.14.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 133/ 273]                 blk.14.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 134/ 273]                 blk.14.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 135/ 273]               blk.14.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 136/ 273]               blk.14.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 137/ 273]               blk.14.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 138/ 273]                 blk.14.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 139/ 273]                 blk.15.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 140/ 273]              blk.15.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 141/ 273]            blk.15.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 142/ 273]                 blk.15.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 143/ 273]                 blk.15.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 144/ 273]               blk.15.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 145/ 273]               blk.15.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 146/ 273]               blk.15.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 147/ 273]                 blk.15.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 148/ 273]                 blk.16.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 149/ 273]              blk.16.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 150/ 273]            blk.16.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 151/ 273]                 blk.16.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 152/ 273]                 blk.16.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 153/ 273]               blk.16.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 154/ 273]               blk.16.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 155/ 273]               blk.16.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 156/ 273]                 blk.16.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 157/ 273]                 blk.17.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 158/ 273]              blk.17.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 159/ 273]            blk.17.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 160/ 273]                 blk.17.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 161/ 273]                 blk.17.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 162/ 273]               blk.17.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 163/ 273]               blk.17.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 164/ 273]               blk.17.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 165/ 273]                 blk.17.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 166/ 273]                 blk.18.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 167/ 273]              blk.18.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 168/ 273]            blk.18.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 169/ 273]                 blk.18.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 170/ 273]                 blk.18.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 171/ 273]               blk.18.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 172/ 273]               blk.18.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 173/ 273]               blk.18.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 174/ 273]                 blk.18.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 175/ 273]                 blk.19.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 176/ 273]              blk.19.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 177/ 273]            blk.19.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 178/ 273]                 blk.19.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 179/ 273]                 blk.19.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 180/ 273]               blk.19.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 181/ 273]               blk.19.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 182/ 273]               blk.19.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 183/ 273]                 blk.19.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 184/ 273]                 blk.20.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 185/ 273]              blk.20.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 186/ 273]            blk.20.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 187/ 273]                 blk.20.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 188/ 273]                 blk.20.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 189/ 273]               blk.20.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 190/ 273]               blk.20.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 191/ 273]               blk.20.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 192/ 273]                 blk.20.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 193/ 273]                 blk.21.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 194/ 273]              blk.21.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 195/ 273]            blk.21.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 196/ 273]                 blk.21.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 197/ 273]                 blk.21.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 198/ 273]               blk.21.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 199/ 273]               blk.21.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 200/ 273]               blk.21.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 201/ 273]                 blk.21.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 202/ 273]                 blk.22.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 203/ 273]              blk.22.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 204/ 273]            blk.22.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 205/ 273]                 blk.22.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 206/ 273]                 blk.22.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 207/ 273]               blk.22.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 208/ 273]               blk.22.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 209/ 273]               blk.22.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 210/ 273]                 blk.22.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 211/ 273]                 blk.23.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 212/ 273]              blk.23.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 213/ 273]            blk.23.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 214/ 273]                 blk.23.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 215/ 273]                 blk.23.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 216/ 273]               blk.23.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 217/ 273]               blk.23.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 218/ 273]               blk.23.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 219/ 273]                 blk.23.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 220/ 273]                 blk.24.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 221/ 273]              blk.24.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 222/ 273]            blk.24.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 223/ 273]                 blk.24.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 224/ 273]                 blk.24.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 225/ 273]               blk.24.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 226/ 273]               blk.24.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 227/ 273]               blk.24.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 228/ 273]                 blk.24.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 229/ 273]                 blk.25.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 230/ 273]              blk.25.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 231/ 273]            blk.25.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 232/ 273]                 blk.25.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 233/ 273]                 blk.25.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 234/ 273]               blk.25.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 235/ 273]               blk.25.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 236/ 273]               blk.25.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 237/ 273]                 blk.25.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 238/ 273]                 blk.26.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 239/ 273]              blk.26.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 240/ 273]            blk.26.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 241/ 273]                 blk.26.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 242/ 273]                 blk.26.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 243/ 273]               blk.26.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 244/ 273]               blk.26.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 245/ 273]               blk.26.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 246/ 273]                 blk.26.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 247/ 273]                 blk.27.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 248/ 273]              blk.27.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 249/ 273]            blk.27.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 250/ 273]                 blk.27.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 251/ 273]                 blk.27.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 252/ 273]               blk.27.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 253/ 273]               blk.27.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 254/ 273]               blk.27.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 255/ 273]                 blk.27.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 256/ 273]                 blk.28.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 257/ 273]              blk.28.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 258/ 273]            blk.28.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 259/ 273]                 blk.28.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 260/ 273]                 blk.28.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 261/ 273]               blk.28.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 262/ 273]               blk.28.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 263/ 273]               blk.28.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 264/ 273]                 blk.28.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 265/ 273]                 blk.29.attn_k.weight - [ 2560,   640,     1,     1], type =    f16, converting to q4_K .. size =     3.12 MiB ->     0.88 MiB\n",
            "[ 266/ 273]              blk.29.attn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 267/ 273]            blk.29.attn_output.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 268/ 273]                 blk.29.attn_q.weight - [ 2560,  2560,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 269/ 273]                 blk.29.attn_v.weight - [ 2560,   640,     1,     1], type =    f16, converting to q6_K .. size =     3.12 MiB ->     1.28 MiB\n",
            "[ 270/ 273]               blk.29.ffn_down.weight - [ 7168,  2560,     1,     1], type =    f16, converting to q6_K .. size =    35.00 MiB ->    14.36 MiB\n",
            "[ 271/ 273]               blk.29.ffn_gate.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "[ 272/ 273]               blk.29.ffn_norm.weight - [ 2560,     1,     1,     1], type =    f32, size =    0.010 MiB\n",
            "[ 273/ 273]                 blk.29.ffn_up.weight - [ 2560,  7168,     1,     1], type =    f16, converting to q4_K .. size =    35.00 MiB ->     9.84 MiB\n",
            "llama_model_quantize_impl: model size  =  4588.10 MiB\n",
            "llama_model_quantize_impl: quant size  =  1424.09 MiB\n",
            "\n",
            "main: quantize time = 53224.34 ms\n",
            "main:    total time = 53224.34 ms\n",
            "Q4_K_M GGUF 생성: True -> /content/exaone35-2.4b-Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "GGUF_F16 = \"/content/exaone35-2.4b-f16.gguf\"\n",
        "GGUF_Q4  = \"/content/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "QUANT    = \"/content/llama.cpp/build/bin/llama-quantize\" \n",
        "\n",
        "!chmod +x \"{QUANT}\"\n",
        "!\"{QUANT}\" \"{GGUF_F16}\" \"{GGUF_Q4}\" q4_K_M\n",
        "\n",
        "import os\n",
        "print(\"Q4_K_M GGUF 생성:\", os.path.exists(GGUF_Q4), \"->\", GGUF_Q4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSMucpQoAIZx",
        "outputId": "cc20a4f7-b5ad-438d-f4e1-67c8132d37ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found llama-cli: ['/content/llama.cpp/build/bin/llama-cli']\n",
            "build: 6727 (cdb6da46) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 273 tensors from /content/exaone35-2.4b-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = exaone\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Sft_Tuned_Model_Epoch_4\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 2.4B\n",
            "llama_model_loader: - kv   4:                    exaone.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   5:                exaone.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   6:             exaone.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   7:                      exaone.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:    exaone.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                 exaone.feed_forward_length u32              = 7168\n",
            "llama_model_loader: - kv  10:                         exaone.block_count u32              = 30\n",
            "llama_model_loader: - kv  11:                      exaone.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  12:                exaone.rope.dimension_count u32              = 80\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = exaone\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,102400]  = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,102400]  = [3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,101782]  = [\"t h\", \"Ġ a\", \"Ġ í\", \"i n\", \"Ġ t...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 361\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  24:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   62 tensors\n",
            "llama_model_loader: - type q4_K:  182 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.39 GiB (4.97 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "\u001b[0mload: printing all EOG tokens:\n",
            "load:   - 42 ('<|endoftext|>')\n",
            "load:   - 361 ('[|endofturn|]')\n",
            "load: special tokens cache size = 362\n",
            "load: token to piece cache size = 0.6622 MB\n",
            "print_info: arch             = exaone\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 30\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 80\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 640\n",
            "print_info: n_embd_v_gqa     = 640\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 7168\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 2.41 B\n",
            "print_info: general.name     = Sft_Tuned_Model_Epoch_4\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 102400\n",
            "print_info: n_merges         = 101782\n",
            "print_info: BOS token        = 1 '[BOS]'\n",
            "print_info: EOS token        = 361 '[|endofturn|]'\n",
            "print_info: EOT token        = 42 '<|endoftext|>'\n",
            "print_info: UNK token        = 3 '[UNK]'\n",
            "print_info: PAD token        = 0 '[PAD]'\n",
            "print_info: LF token         = 560 'Ċ'\n",
            "print_info: EOG token        = 42 '<|endoftext|>'\n",
            "print_info: EOG token        = 361 '[|endofturn|]'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:   CPU_REPACK model buffer size =   999.49 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1414.24 MiB\n",
            "......................................................................................\n",
            "llama_init_from_model: model default pooling_type is [0], but [-1] was specified\n",
            "\u001b[0mllama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = auto\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "\u001b[0mllama_context:        CPU  output buffer size =     0.39 MiB\n",
            "llama_kv_cache:        CPU KV buffer size =   300.00 MiB\n",
            "llama_kv_cache: size =  300.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):  150.00 MiB, V (f16):  150.00 MiB\n",
            "llama_context: Flash Attention was auto, set to enabled\n",
            "llama_context:        CPU compute buffer size =   205.00 MiB\n",
            "llama_context: graph nodes  = 937\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added [|endofturn|] logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "\u001b[0mmain: llama threadpool init, n_threads = 6\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n",
            "\u001b[0mmain: chat template example:\n",
            "[|system|]You are a helpful assistant[|endofturn|]\n",
            "[|user|]Hello\n",
            "[|assistant|]Hi there[|endofturn|]\n",
            "[|user|]How are you?\n",
            "[|assistant|]\n",
            "\n",
            "system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "Reverse prompt: '### 응답:'\n",
            "Reverse prompt: '\n",
            "사용자:'\n",
            "Reverse prompt: '\n",
            "시스템:'\n",
            "Reverse prompt: '\n",
            "어시스턴트:'\n",
            "sampler seed: 1233737864\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.900, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 0\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            "[|user|]시스템: 너는 세종대왕 말투로, 간결하고 품위 있게 대답한다.\n",
            "사용자: 오늘 할 일을 세 줄로 정리해주시오.\n",
            "어시스턴트:\n",
            "[|assistant|]백성을 위해 법을 정비하고, 학문을 장려하며, 나라의 기틀을 세우는 일에 힘쓰리라. 이 세 가지에 힘을 쏟을 것이니라.\n",
            "\n",
            "> "
          ]
        }
      ],
      "source": [
        "import glob, os\n",
        "\n",
        "GGUF_Q4 = \"/content/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "\n",
        "# llama-cli 위치 자동 탐색 (build/bin 또는 bin 등)\n",
        "cli_candidates = glob.glob(\"/content/llama.cpp/**/llama-cli\", recursive=True)\n",
        "print(\"Found llama-cli:\", cli_candidates)\n",
        "assert cli_candidates, \"llama-cli 실행파일을 찾지 못했습니다. 빌드가 완료되었는지 확인하세요.\"\n",
        "\n",
        "CLI = cli_candidates[0]\n",
        "!chmod +x \"{CLI}\"\n",
        "\n",
        "prompt = \"시스템: 너는 세종대왕 말투로, 간결하고 품위 있게 대답한다.\\n사용자: 오늘 할 일을 세 줄로 정리해주시오.\\n어시스턴트:\"\n",
        "\n",
        "!\"{CLI}\" -m \"{GGUF_Q4}\" \\\n",
        "  -p \"{prompt}\" \\\n",
        "  -n 128 -c 4096 --temp 0.7 --top-p 0.9 \\\n",
        "  -r \"### 응답:\" -r \"\\n사용자:\" -r \"\\n시스템:\" -r \"\\n어시스턴트:\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95C34SUtAKM4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil, os\n",
        "GGUF_Q4      = \"/content/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "GGUF_BACKUP  = \"/content/drive/MyDrive/exaone35-2.4b-Q4_K_M.gguf\"\n",
        "\n",
        "if os.path.exists(GGUF_Q4):\n",
        "    shutil.copy(GGUF_Q4, GGUF_BACKUP)\n",
        "    print(\"✅ Drive 백업 완료:\", GGUF_BACKUP)\n",
        "else:\n",
        "    print(\"❌ GGUF가 없습니다. 양자화 단계가 성공했는지 확인하세요.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
